{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df66aa61",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "df66aa61"
      },
      "outputs": [],
      "source": [
        "!pip install PyMupdf transformers sentence-transformers faiss-cpu requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a05f7ef",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "6a05f7ef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import requests # For making HTTP requests to the Gemini API\n",
        "import json # For handling JSON data\n",
        "from typing import List, Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cc79200",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "4cc79200"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cb8f07f",
      "metadata": {
        "id": "1cb8f07f"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "211b2b3b",
      "metadata": {
        "id": "211b2b3b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 2. Document Loading and Chunking ---\n",
        "def load_documents(filepath: str) -> List[str]:\n",
        "    \"\"\"Loads text documents from a file.\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Error: Document file not found at {filepath}\")\n",
        "        return []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "    # Simple splitting by double newline for paragraphs/sections\n",
        "    documents = [doc.strip() for doc in content.split('\\n\\n') if doc.strip()]\n",
        "    print(f\"Loaded {len(documents)} documents.\")\n",
        "    return documents\n",
        "\n",
        "def chunk_document(document: str, chunk_size: int = 256, overlap: int = 50) -> List[str]:\n",
        "    \"\"\"\n",
        "    Splits a single document into smaller chunks.\n",
        "    This is a basic character-based splitter. For production, consider token-based.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    if not document:\n",
        "        return chunks\n",
        "\n",
        "    words = document.split()\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for word in words:\n",
        "        if current_length + len(word) + 1 <= chunk_size:\n",
        "            current_chunk.append(word)\n",
        "            current_length += len(word) + 1\n",
        "        else:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            # Start new chunk with overlap\n",
        "            overlap_words = current_chunk[max(0, len(current_chunk) - overlap):]\n",
        "            current_chunk = overlap_words + [word]\n",
        "            current_length = sum(len(w) + 1 for w in current_chunk)\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    print(f\"Document chunked into {len(chunks)} pieces.\")\n",
        "    return chunks\n",
        "\n",
        "# Create a dummy document file for demonstration\n",
        "dummy_text = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog. This is a classic sentence\n",
        "used to demonstrate all letters of the alphabet. It's often used in\n",
        "typing tests and font demonstrations.\n",
        "\n",
        "Artificial intelligence (AI) is intelligence demonstrated by machines,\n",
        "unlike the natural intelligence displayed by humans and animals.\n",
        "Leading AI textbooks define the field as the study of \"intelligent agents\":\n",
        "any device that perceives its environment and takes actions that maximize\n",
        "its chance of successfully achieving its goals.\n",
        "\n",
        "Machine learning (ML) is a subset of artificial intelligence that\n",
        "focuses on the development of algorithms that allow computers to learn\n",
        "from and make predictions or decisions based on data. It is a core component\n",
        "of many modern AI applications.\n",
        "\n",
        "Natural Language Processing (NLP) is a subfield of artificial intelligence\n",
        "that deals with the interaction between computers and human language.\n",
        "It involves enabling computers to understand, interpret, and generate human language.\n",
        "NLP applications include machine translation, spam detection, and sentiment analysis.\n",
        "\"\"\"\n",
        "with open('sample_documents.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(dummy_text)\n",
        "\n",
        "# Load and chunk our documents\n",
        "documents = load_documents('sample_documents.txt')\n",
        "all_chunks = []\n",
        "for doc in documents:\n",
        "    all_chunks.extend(chunk_document(doc))\n",
        "\n",
        "print(f\"Total chunks created: {len(all_chunks)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cdabbc9",
      "metadata": {
        "id": "1cdabbc9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 3. Embedding Generation and Vector Store (FAISS) ---\n",
        "class VectorStore:\n",
        "    def __init__(self, embedding_model_name: str):\n",
        "        \"\"\"Initializes the embedding model and FAISS index.\"\"\"\n",
        "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
        "        self.index = None\n",
        "        self.chunk_map = [] # To map FAISS index to original text chunks\n",
        "        print(f\"Embedding model '{embedding_model_name}' loaded.\")\n",
        "\n",
        "    def build_index(self, chunks: List[str]):\n",
        "        \"\"\"Generates embeddings and builds the FAISS index.\"\"\"\n",
        "        print(\"Generating embeddings for chunks...\")\n",
        "        self.chunk_map = chunks\n",
        "        chunk_embeddings = self.embedding_model.encode(chunks, convert_to_tensor=True)\n",
        "        # Move embeddings to CPU and convert to numpy for FAISS\n",
        "        chunk_embeddings_np = chunk_embeddings.cpu().numpy().astype('float32')\n",
        "\n",
        "        # Get embedding dimension\n",
        "        embedding_dim = chunk_embeddings_np.shape[1]\n",
        "\n",
        "        # Initialize FAISS index (Flat index for exact search)\n",
        "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
        "        self.index.add(chunk_embeddings_np)\n",
        "        print(f\"FAISS index built with {self.index.ntotal} vectors.\")\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 3) -> List[str]:\n",
        "        \"\"\"Retrieves top-k most relevant chunks based on query.\"\"\"\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"FAISS index not built. Call build_index() first.\")\n",
        "\n",
        "        query_embedding = self.embedding_model.encode([query], convert_to_tensor=True)\n",
        "        query_embedding_np = query_embedding.cpu().numpy().astype('float32')\n",
        "\n",
        "        # Search the index\n",
        "        distances, indices = self.index.search(query_embedding_np, k)\n",
        "\n",
        "        retrieved_chunks = [self.chunk_map[idx] for idx in indices[0]]\n",
        "        print(f\"Retrieved {len(retrieved_chunks)} chunks for query.\")\n",
        "        return retrieved_chunks\n",
        "\n",
        "# Initialize and build the vector store\n",
        "vector_store = VectorStore(EMBEDDING_MODEL_NAME)\n",
        "vector_store.build_index(all_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d772372",
      "metadata": {
        "id": "4d772372"
      },
      "outputs": [],
      "source": [
        "# --- 4. Retrieval Augmented Generation (RAG) System ---\n",
        "class NonPromptBasedRAG:\n",
        "    def __init__(self, vector_store: VectorStore):\n",
        "        self.vector_store = vector_store\n",
        "        print(\"NonPromptBasedRAG system initialized.\")\n",
        "\n",
        "    def generate_response(self, query: str, num_retrieved_chunks: int = 3) -> str:\n",
        "        \"\"\"\n",
        "        Performs retrieval first, then uses the retrieved context for generation\n",
        "        using the Google Gemini API.\n",
        "        \"\"\"\n",
        "        print(f\"\\nProcessing query: '{query}'\")\n",
        "\n",
        "        # Step 1: Retrieve relevant documents (non-prompt based retrieval)\n",
        "        retrieved_context_chunks = self.vector_store.retrieve(query, k=num_retrieved_chunks)\n",
        "        context = \"\\n\".join(retrieved_context_chunks)\n",
        "\n",
        "        print(\"\\n--- Retrieved Context ---\")\n",
        "        for i, chunk in enumerate(retrieved_context_chunks):\n",
        "            print(f\"Chunk {i+1}:\\n{chunk}\\n---\")\n",
        "\n",
        "        # Step 2: Formulate the prompt for the LLM using the retrieved context\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are a helpful AI assistant. Use the following context to answer the user's question.\n",
        "        If you don't know the answer based on the context, just say you don't know.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {query}\n",
        "\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"\\n--- Prompt sent to LLM ---\\n{prompt}\\n---\")\n",
        "\n",
        "        # Step 3: Send the prompt to the Google Gemini API\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json',\n",
        "        }\n",
        "        params = {\n",
        "            'key': GEMINI_API_KEY,\n",
        "        }\n",
        "        payload = {\n",
        "            \"contents\": [\n",
        "                {\n",
        "                    \"parts\": [\n",
        "                        {\"text\": prompt}\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        response = requests.post(GEMINI_API_URL, headers=headers, params=params, data=json.dumps(payload))\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            try:\n",
        "                response_json = response.json()\n",
        "                # Navigate through the JSON structure to get the text\n",
        "                generated_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
        "                print(\"\\n--- Generated Response ---\")\n",
        "                print(generated_text)\n",
        "                return generated_text\n",
        "            except (KeyError, IndexError, json.JSONDecodeError) as e:\n",
        "                print(f\"Error parsing Gemini API response: {e}\")\n",
        "                print(\"Full API Response:\", response.text)\n",
        "                return \"Error: Could not parse response from language model.\"\n",
        "        else:\n",
        "            print(f\"Error from Gemini API: Status Code {response.status_code}\")\n",
        "            print(\"Error details:\", response.text)\n",
        "            return f\"Error: Language model API request failed (Status Code: {response.status_code}).\"\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "# Initialize the RAG system\n",
        "rag_system = NonPromptBasedRAG(vector_store)\n",
        "\n",
        "# Ask a question\n",
        "query = \"What is Artificial Intelligence?\"\n",
        "response = rag_system.generate_response(query)\n",
        "\n",
        "print(f\"\\nFinal Answer for query '{query}':\")\n",
        "print(response)\n",
        "\n",
        "query = \"What is NLP?\"\n",
        "response = rag_system.generate_response(query)\n",
        "\n",
        "print(f\"\\nFinal Answer for query '{query}':\")\n",
        "print(response)\n",
        "\n",
        "query = \"Tell me about the quick brown fox.\"\n",
        "response = rag_system.generate_response(query)\n",
        "\n",
        "print(f\"\\nFinal Answer for query '{query}':\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dd9d6c8"
      },
      "source": [
        "# Task\n",
        "Integrate and run Ragas evaluation metrics (context relevancy, answer relevancy, and faithfulness) on the provided RAG code."
      ],
      "id": "3dd9d6c8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70ce1570"
      },
      "source": [
        "## Install ragas\n",
        "\n",
        "### Subtask:\n",
        "Install the Ragas library and any necessary dependencies.\n"
      ],
      "id": "70ce1570"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc99d832"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to install the `ragas` library. This can be done using pip in a code cell.\n",
        "\n"
      ],
      "id": "bc99d832"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e900f7f"
      },
      "source": [
        "!pip install ragas"
      ],
      "id": "9e900f7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31edd26a"
      },
      "source": [
        "## Integrate ragas evaluation\n",
        "\n",
        "### Subtask:\n",
        "Modify the existing RAG system to incorporate Ragas evaluation metrics, specifically context relevancy, answer relevancy, and faithfulness. This will involve preparing the data in the format required by Ragas.\n"
      ],
      "id": "31edd26a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "715c9a04"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the existing RAG system to collect the necessary data for Ragas evaluation, specifically questions, generated answers, and retrieved contexts, and store them in the required format. Then import the necessary Ragas classes and prepare the dataset for evaluation.\n",
        "\n"
      ],
      "id": "715c9a04"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01d70b95"
      },
      "source": [
        "from ragas.metrics import faithfulness, answer_relevancy, context_relevancy\n",
        "from ragas import evaluate\n",
        "from datasets import Dataset\n",
        "\n",
        "# 1. Define a list of questions for evaluation\n",
        "evaluation_questions = [\n",
        "    \"What is Artificial Intelligence?\",\n",
        "    \"What is Machine Learning?\",\n",
        "    \"What is NLP?\",\n",
        "    \"Tell me about the quick brown fox.\",\n",
        "    \"What is the capital of France?\" # A question outside the document context\n",
        "]\n",
        "\n",
        "# 2 & 3. Collect data for Ragas evaluation\n",
        "evaluation_data = []\n",
        "for question in evaluation_questions:\n",
        "    # We need to modify the generate_response method slightly to return retrieved chunks\n",
        "    # For this example, we will call retrieve separately and then generate the response\n",
        "    num_retrieved_chunks = 3 # Same as in the RAG system\n",
        "    retrieved_context_chunks = vector_store.retrieve(question, k=num_retrieved_chunks)\n",
        "    context = \"\\n\".join(retrieved_context_chunks)\n",
        "\n",
        "    # Call the generate_response to get the answer.\n",
        "    # Note: The current generate_response prints the context and prompt, which is fine for now.\n",
        "    # In a real scenario, you might want a separate method for evaluation data collection\n",
        "    # that doesn't print as much.\n",
        "    generated_answer = rag_system.generate_response(question, num_retrieved_chunks)\n",
        "\n",
        "    evaluation_data.append({\n",
        "        'question': question,\n",
        "        'answer': generated_answer,\n",
        "        'contexts': retrieved_context_chunks # Ragas expects a list of strings for contexts\n",
        "    })\n",
        "\n",
        "# 4 & 5. Instantiate Ragas evaluation metrics (already imported above)\n",
        "# Metrics will be instantiated later when calling evaluate\n",
        "\n",
        "# 6. Create a Ragas Dataset object\n",
        "# Convert the list of dictionaries to a Hugging Face Dataset\n",
        "eval_dataset = Dataset.from_list(evaluation_data)\n",
        "\n",
        "print(\"\\n--- Evaluation Dataset Sample ---\")\n",
        "print(eval_dataset)"
      ],
      "id": "01d70b95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c38318e7"
      },
      "source": [
        "## Run ragas evaluation\n",
        "\n",
        "### Subtask:\n",
        "Execute the Ragas evaluation pipeline on the generated responses and the retrieved context.\n"
      ],
      "id": "c38318e7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad570089"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the metrics and call the `evaluate` function from ragas with the created dataset and metrics.\n",
        "\n"
      ],
      "id": "ad570089"
    },
    {
      "cell_type": "code",
      "source": [
        "rom ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "\n",
        "evaluation_results = evaluate(\n",
        "    dataset = eval_dataset,\n",
        "    metrics=[\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "    ],\n",
        ")\n",
        "\n",
        "df = evaluation_results.to_pandas()\n",
        "# Print the evaluation results\n",
        "print(\"\\n--- Ragas Evaluation Results ---\")\n",
        "print(evaluation_results)"
      ],
      "metadata": {
        "id": "So2AWBQoxv0P"
      },
      "id": "So2AWBQoxv0P",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}