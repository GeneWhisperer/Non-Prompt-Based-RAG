{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66aa61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install PyMupdf transformers sentence-transformers faiss-cpu requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05f7ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests # For making HTTP requests to the Gemini API\n",
    "import json # For handling JSON data\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc79200",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb8f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211b2b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Document Loading and Chunking ---\n",
    "def load_documents(filepath: str) -> List[str]:\n",
    "    \"\"\"Loads text documents from a file.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Document file not found at {filepath}\")\n",
    "        return []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    # Simple splitting by double newline for paragraphs/sections\n",
    "    documents = [doc.strip() for doc in content.split('\\n\\n') if doc.strip()]\n",
    "    print(f\"Loaded {len(documents)} documents.\")\n",
    "    return documents\n",
    "\n",
    "def chunk_document(document: str, chunk_size: int = 256, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a single document into smaller chunks.\n",
    "    This is a basic character-based splitter. For production, consider token-based.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    if not document:\n",
    "        return chunks\n",
    "\n",
    "    words = document.split()\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        if current_length + len(word) + 1 <= chunk_size:\n",
    "            current_chunk.append(word)\n",
    "            current_length += len(word) + 1\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            # Start new chunk with overlap\n",
    "            overlap_words = current_chunk[max(0, len(current_chunk) - overlap):]\n",
    "            current_chunk = overlap_words + [word]\n",
    "            current_length = sum(len(w) + 1 for w in current_chunk)\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    print(f\"Document chunked into {len(chunks)} pieces.\")\n",
    "    return chunks\n",
    "\n",
    "# Create a dummy document file for demonstration\n",
    "dummy_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. This is a classic sentence\n",
    "used to demonstrate all letters of the alphabet. It's often used in\n",
    "typing tests and font demonstrations.\n",
    "\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines,\n",
    "unlike the natural intelligence displayed by humans and animals.\n",
    "Leading AI textbooks define the field as the study of \"intelligent agents\":\n",
    "any device that perceives its environment and takes actions that maximize\n",
    "its chance of successfully achieving its goals.\n",
    "\n",
    "Machine learning (ML) is a subset of artificial intelligence that\n",
    "focuses on the development of algorithms that allow computers to learn\n",
    "from and make predictions or decisions based on data. It is a core component\n",
    "of many modern AI applications.\n",
    "\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence\n",
    "that deals with the interaction between computers and human language.\n",
    "It involves enabling computers to understand, interpret, and generate human language.\n",
    "NLP applications include machine translation, spam detection, and sentiment analysis.\n",
    "\"\"\"\n",
    "with open('sample_documents.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(dummy_text)\n",
    "\n",
    "# Load and chunk our documents\n",
    "documents = load_documents('sample_documents.txt')\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    all_chunks.extend(chunk_document(doc))\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdabbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. Embedding Generation and Vector Store (FAISS) ---\n",
    "class VectorStore:\n",
    "    def __init__(self, embedding_model_name: str):\n",
    "        \"\"\"Initializes the embedding model and FAISS index.\"\"\"\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        self.index = None\n",
    "        self.chunk_map = [] # To map FAISS index to original text chunks\n",
    "        print(f\"Embedding model '{embedding_model_name}' loaded.\")\n",
    "\n",
    "    def build_index(self, chunks: List[str]):\n",
    "        \"\"\"Generates embeddings and builds the FAISS index.\"\"\"\n",
    "        print(\"Generating embeddings for chunks...\")\n",
    "        self.chunk_map = chunks\n",
    "        chunk_embeddings = self.embedding_model.encode(chunks, convert_to_tensor=True)\n",
    "        # Move embeddings to CPU and convert to numpy for FAISS\n",
    "        chunk_embeddings_np = chunk_embeddings.cpu().numpy().astype('float32')\n",
    "\n",
    "        # Get embedding dimension\n",
    "        embedding_dim = chunk_embeddings_np.shape[1]\n",
    "\n",
    "        # Initialize FAISS index (Flat index for exact search)\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        self.index.add(chunk_embeddings_np)\n",
    "        print(f\"FAISS index built with {self.index.ntotal} vectors.\")\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[str]:\n",
    "        \"\"\"Retrieves top-k most relevant chunks based on query.\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"FAISS index not built. Call build_index() first.\")\n",
    "\n",
    "        query_embedding = self.embedding_model.encode([query], convert_to_tensor=True)\n",
    "        query_embedding_np = query_embedding.cpu().numpy().astype('float32')\n",
    "\n",
    "        # Search the index\n",
    "        distances, indices = self.index.search(query_embedding_np, k)\n",
    "\n",
    "        retrieved_chunks = [self.chunk_map[idx] for idx in indices[0]]\n",
    "        print(f\"Retrieved {len(retrieved_chunks)} chunks for query.\")\n",
    "        return retrieved_chunks\n",
    "\n",
    "# Initialize and build the vector store\n",
    "vector_store = VectorStore(EMBEDDING_MODEL_NAME)\n",
    "vector_store.build_index(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d772372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Retrieval Augmented Generation (RAG) System ---\n",
    "class NonPromptBasedRAG:\n",
    "    def __init__(self, vector_store: VectorStore):\n",
    "        self.vector_store = vector_store\n",
    "        print(\"NonPromptBasedRAG system initialized.\")\n",
    "\n",
    "    def generate_response(self, query: str, num_retrieved_chunks: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Performs retrieval first, then uses the retrieved context for generation\n",
    "        using the Google Gemini API.\n",
    "        \"\"\"\n",
    "        print(f\"\\nProcessing query: '{query}'\")\n",
    "\n",
    "        # Step 1: Retrieve relevant documents (non-prompt based retrieval)\n",
    "        retrieved_context_chunks = self.vector_store.retrieve(query, k=num_retrieved_chunks)\n",
    "        context = \"\\n\".join(retrieved_context_chunks)\n",
    "\n",
    "        print(\"\\n--- Retrieved Context ---\")\n",
    "        for i, chunk in enumerate(retrieved_context_chunks):\n",
    "            print(f\"Chunk {i+1}:\\n{chunk}\\n---\")\n",
    "\n",
    "        # Step 2: Formulate the prompt for the LLM using the retrieved context\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful AI assistant. Use the following context to answer the user's question.\n",
    "        If you don't know the answer based on the context, just say you don't know.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {query}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"\\n--- Prompt sent to LLM ---\\n{prompt}\\n---\")\n",
    "\n",
    "        # Step 3: Send the prompt to the Google Gemini API\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "        }\n",
    "        params = {\n",
    "            'key': GEMINI_API_KEY,\n",
    "        }\n",
    "        payload = {\n",
    "            \"contents\": [\n",
    "                {\n",
    "                    \"parts\": [\n",
    "                        {\"text\": prompt}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        response = requests.post(GEMINI_API_URL, headers=headers, params=params, data=json.dumps(payload))\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                response_json = response.json()\n",
    "                # Navigate through the JSON structure to get the text\n",
    "                generated_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
    "                print(\"\\n--- Generated Response ---\")\n",
    "                print(generated_text)\n",
    "                return generated_text\n",
    "            except (KeyError, IndexError, json.JSONDecodeError) as e:\n",
    "                print(f\"Error parsing Gemini API response: {e}\")\n",
    "                print(\"Full API Response:\", response.text)\n",
    "                return \"Error: Could not parse response from language model.\"\n",
    "        else:\n",
    "            print(f\"Error from Gemini API: Status Code {response.status_code}\")\n",
    "            print(\"Error details:\", response.text)\n",
    "            return f\"Error: Language model API request failed (Status Code: {response.status_code}).\"\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# Initialize the RAG system\n",
    "rag_system = NonPromptBasedRAG(vector_store)\n",
    "\n",
    "# Ask a question\n",
    "query = \"What is Artificial Intelligence?\"\n",
    "response = rag_system.generate_response(query)\n",
    "\n",
    "print(f\"\\nFinal Answer for query '{query}':\")\n",
    "print(response)\n",
    "\n",
    "query = \"What is NLP?\"\n",
    "response = rag_system.generate_response(query)\n",
    "\n",
    "print(f\"\\nFinal Answer for query '{query}':\")\n",
    "print(response)\n",
    "\n",
    "query = \"Tell me about the quick brown fox.\"\n",
    "response = rag_system.generate_response(query)\n",
    "\n",
    "print(f\"\\nFinal Answer for query '{query}':\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
