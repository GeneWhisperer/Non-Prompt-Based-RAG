{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7ab221f"
      },
      "source": [
        "import google.generativeai as genai\n",
        "import numpy as np\n",
        "# from sklearn.metrics.pairwise import cosine_similarity # No longer needed for retrieval with FAISS\n",
        "from IPython.display import Markdown, display\n",
        "import os\n",
        "import faiss # Import FAISS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79ae15ed"
      },
      "source": [
        "# --- 1. Configuration and API Key Setup ---\n",
        "try:\n",
        "    genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
        "    if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "        print(\"Warning: GOOGLE_API_KEY environment variable not set. Please set it for API access.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring Gemini API: {e}\")\n",
        "    raise # Re-raise to stop execution if models can't be initialized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b83dfd6"
      },
      "source": [
        "# --- 2. Initialize Models ---\n",
        "GENERATION_MODEL_NAME = 'gemini-1.5-flash-latest'\n",
        "EMBEDDING_MODEL_NAME = 'text-embedding-004'\n",
        "\n",
        "try:\n",
        "    generation_model = genai.GenerativeModel(GENERATION_MODEL_NAME)\n",
        "    embedding_model = genai.GenerativeModel(EMBEDDING_MODEL_NAME)\n",
        "    print(f\"Gemini models initialized: '{GENERATION_MODEL_NAME}' for generation, '{EMBEDDING_MODEL_NAME}' for embeddings.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Gemini models: {e}\")\n",
        "    raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56494797"
      },
      "source": [
        "# --- 3. Create a Simple In-Memory Knowledge Base ---\n",
        "knowledge_base_documents = [\n",
        "    \"The capital of France is Paris. Paris is known for its Eiffel Tower.\",\n",
        "    \"The Amazon rainforest is the largest tropical rainforest in the world.\",\n",
        "    \"Python is a high-level, general-purpose programming language.\",\n",
        "    \"Artificial intelligence (AI) is intelligence demonstrated by machines.\",\n",
        "    \"The human heart has four chambers: two atria and two ventricles.\",\n",
        "    \"The deepest ocean trench is the Mariana Trench, located in the western Pacific Ocean.\",\n",
        "    \"Machine learning is a subset of AI that enables systems to learn from data.\",\n",
        "    \"The Earth revolves around the Sun in an elliptical orbit.\",\n",
        "    \"Quantum computing uses quantum-mechanical phenomena like superposition and entanglement.\",\n",
        "    \"The Great Barrier Reef is the world's largest coral reef system, located off the coast of Queensland, Australia.\"\n",
        "]\n",
        "\n",
        "print(f\"Knowledge base loaded with {len(knowledge_base_documents)} documents.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d1512a0"
      },
      "source": [
        "# --- 4. Embed the Knowledge Base Documents and Build FAISS Index ---\n",
        "\n",
        "# Global variables to store the FAISS index and the mapping of index to document\n",
        "faiss_index = None\n",
        "document_store = [] # This will map the FAISS index to the original documents\n",
        "\n",
        "print(\"Starting document embedding process and FAISS index construction...\")\n",
        "embeddings_list = [] # Temporary list to collect all embeddings\n",
        "\n",
        "for i, doc in enumerate(knowledge_base_documents):\n",
        "    try:\n",
        "        embedding_response = embedding_model.embed_content(model=EMBEDDING_MODEL_NAME, content=doc)\n",
        "        if embedding_response and 'embedding' in embedding_response:\n",
        "            embedding = embedding_response['embedding']\n",
        "            embeddings_list.append(embedding)\n",
        "            document_store.append(doc) # Store the document at the same index\n",
        "            print(f\"Embedded document {i+1}/{len(knowledge_base_documents)}\")\n",
        "        else:\n",
        "            print(f\"Warning: No embedding found for document {i+1}: {doc[:50]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error embedding document {i+1} ('{doc[:50]}...'): {e}\")\n",
        "\n",
        "if not embeddings_list:\n",
        "    raise ValueError(\"No embeddings were successfully generated for the knowledge base. Cannot build FAISS index.\")\n",
        "\n",
        "# Convert list of embeddings to a NumPy array (float32 is standard for FAISS)\n",
        "embeddings_np = np.array(embeddings_list).astype('float32')\n",
        "\n",
        "# Get the dimensionality of the embeddings\n",
        "d = embeddings_np.shape[1]\n",
        "\n",
        "# Initialize a FAISS index. For simplicity, we use IndexFlatL2 for L2 distance (Euclidean).\n",
        "# For production, consider IndexIVFFlat for better performance with large datasets.\n",
        "try:\n",
        "    faiss_index = faiss.IndexFlatL2(d) # L2 distance is common for embeddings\n",
        "    faiss_index.add(embeddings_np) # Add the embeddings to the index\n",
        "    print(f\"FAISS index built with {faiss_index.ntotal} documents and dimensionality {d}.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error building FAISS index: {e}\")\n",
        "    raise # Stop if FAISS index cannot be built"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35b491dc"
      },
      "source": [
        "# --- 5. Retrieval Mechanism (Updated for FAISS) ---\n",
        "def retrieve_relevant_documents(query_embedding, top_k=2):\n",
        "    '''\n",
        "    Retrieves the top_k most relevant documents from the FAISS index\n",
        "    based on similarity with the query embedding.\n",
        "\n",
        "    Args:\n",
        "        query_embedding (np.array): The embedding of the user's query.\n",
        "        top_k (int): The number of top documents to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of relevant documents (strings).\n",
        "    '''\n",
        "    global faiss_index, document_store\n",
        "\n",
        "    if faiss_index is None or faiss_index.ntotal == 0:\n",
        "        print(\"Warning: FAISS index is not initialized or empty, no documents to retrieve.\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        # Reshape query_embedding to be a 2D array (1, d) as required by FAISS search\n",
        "        query_embedding_faiss = query_embedding.reshape(1, -1).astype('float32')\n",
        "\n",
        "        # Perform the search: D are distances, I are indices\n",
        "        # FAISS returns distances (D) and indices (I) of the top_k nearest neighbors\n",
        "        D, I = faiss_index.search(query_embedding_faiss, top_k)\n",
        "\n",
        "        retrieved_docs = []\n",
        "        for idx in I[0]: # I[0] contains the indices for the first (and only) query\n",
        "            if idx != -1: # FAISS returns -1 for empty slots if k > ntotal\n",
        "                retrieved_docs.append(document_store[idx]) # Retrieve the actual document\n",
        "        print(f\"Retrieved {len(retrieved_docs)} documents using FAISS.\")\n",
        "        return retrieved_docs\n",
        "    except Exception as e:\n",
        "        print(f\"Error during FAISS retrieval: {e}\")\n",
        "        return []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78963038"
      },
      "source": [
        "# --- 6. Agentic Logic for RAG (No changes here, as it calls the updated retrieval function) ---\n",
        "def agentic_rag_system(user_query):\n",
        "    '''\n",
        "    An agentic RAG system that decides whether to retrieve information\n",
        "    before generating a response.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's input query.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response from the RAG system.\n",
        "    '''\n",
        "    print(f\"\\n--- User Query: '{user_query}' ---\")\n",
        "\n",
        "    # --- Agent's Decision Phase ---\n",
        "    keywords_for_retrieval = [\"what is\", \"tell me about\", \"where is\", \"who is\", \"explain\", \"describe\", \"facts about\", \"information on\"]\n",
        "    needs_retrieval = any(keyword in user_query.lower() for keyword in keywords_for_retrieval)\n",
        "\n",
        "    retrieved_context = \"\"\n",
        "    if needs_retrieval:\n",
        "        print(\"Agent decision: Retrieval is likely needed based on keywords.\")\n",
        "        try:\n",
        "            query_embedding_response = embedding_model.embed_content(model=EMBEDDING_MODEL_NAME, content=user_query)\n",
        "            if query_embedding_response and 'embedding' in query_embedding_response:\n",
        "                query_embedding = query_embedding_response['embedding']\n",
        "                relevant_docs = retrieve_relevant_documents(np.array(query_embedding))\n",
        "\n",
        "                if relevant_docs:\n",
        "                    retrieved_context = \"\\n\".join(relevant_docs)\n",
        "                    print(\"\\n--- Retrieved Context ---\")\n",
        "                    display(Markdown(f\"```text\\n{retrieved_context}\\n```\"))\n",
        "                else:\n",
        "                    print(\"No relevant documents found for retrieval.\")\n",
        "            else:\n",
        "                print(\"Warning: Could not get embedding for the user query, skipping retrieval.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during query embedding or retrieval: {e}\")\n",
        "            retrieved_context = \"\"\n",
        "    else:\n",
        "        print(\"Agent decision: Direct generation without retrieval (no relevant keywords detected).\")\n",
        "\n",
        "    # --- Generation Phase ---\n",
        "    prompt_parts = []\n",
        "    if retrieved_context:\n",
        "        prompt_parts.append(f\"Here is some relevant information:\\n{retrieved_context}\\n\\n\")\n",
        "    prompt_parts.append(f\"Based on the provided information (if any) and your general knowledge, please answer the following question:\\nQuestion: {user_query}\\n\\n\")\n",
        "    prompt_parts.append(\"Answer:\")\n",
        "\n",
        "    final_prompt = \"\".join(prompt_parts)\n",
        "    print(f\"\\n--- Final Prompt Sent to Gemini ---\\n```\\n{final_prompt}\\n```\")\n",
        "\n",
        "    try:\n",
        "        response = generation_model.generate_content(final_prompt)\n",
        "\n",
        "        if response.candidates and response.candidates[0].content:\n",
        "            generated_text = response.candidates[0].content.parts[0].text\n",
        "            print(\"\\n--- Generated Response ---\")\n",
        "            display(Markdown(generated_text))\n",
        "        else:\n",
        "            print(\"\\n--- No Valid Response from Gemini ---\")\n",
        "            generated_text = \"I couldn't generate a valid response for that query.\"\n",
        "            if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
        "                print(f\"Blocked reason: {response.prompt_feedback.block_reason.name}\")\n",
        "                generated_text += f\"\\nReason: {response.prompt_feedback.block_reason.name}\"\n",
        "            elif response.candidates and response.candidates[0].finish_reason:\n",
        "                print(f\"Finish reason: {response.candidates[0].finish_reason.name}\")\n",
        "                generated_text += f\"\\nFinish Reason: {response.candidates[0].finish_reason.name}\"\n",
        "            display(Markdown(generated_text))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- Error Generating Content with Gemini ---\")\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        generated_text = \"An error occurred while generating the response.\"\n",
        "        display(Markdown(generated_text))\n",
        "\n",
        "    return generated_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef4dd68e"
      },
      "source": [
        "# --- Example Usage ---\n",
        "print(\"\\n--- Running Example Queries ---\")\n",
        "agentic_rag_system(\"What is the capital of France?\")\n",
        "agentic_rag_system(\"Tell me about quantum computing.\")\n",
        "agentic_rag_system(\"What is the largest tropical rainforest?\")\n",
        "agentic_rag_system(\"How many chambers does the human heart have?\")\n",
        "agentic_rag_system(\"What is the deepest ocean trench?\")\n",
        "agentic_rag_system(\"Tell me about the universe.\")\n",
        "agentic_rag_system(\"Define artificial intelligence.\")\n",
        "agentic_rag_system(\"Explain machine learning.\")\n",
        "agentic_rag_system(\"Tell me about the Big Bang theory.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c945d89"
      },
      "source": [
        "# Task\n",
        "Implement Ragas metrics (context relevancy, answer relevancy, and faithfulness) for evaluating a RAG system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54508012"
      },
      "source": [
        "## Install ragas\n",
        "\n",
        "### Subtask:\n",
        "Install the ragas library and any dependencies it requires.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bba8686"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to install the `ragas` library and its dependencies. This requires using a package manager like pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ee21413"
      },
      "source": [
        "%pip install ragas datasets langchain --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8364ef0"
      },
      "source": [
        "## Prepare Evaluation Data\n",
        "\n",
        "### Subtask:\n",
        "Create a dataset with queries, retrieved contexts, generated answers, and ground truth answers.\n",
        "\n",
        "**Reasoning**:\n",
        "To evaluate the RAG system using Ragas, we need to prepare a dataset that contains the necessary components for each query: the query itself, the context retrieved by the system, the answer generated by the system, and a ground truth answer for comparison. This dataset will be used as input for the Ragas evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8bd5e50"
      },
      "source": [
        "# --- 2. Prepare Evaluation Data ---\n",
        "\n",
        "# Define a list of queries for evaluation\n",
        "eval_queries = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Tell me about quantum computing.\",\n",
        "    \"What is the largest tropical rainforest?\",\n",
        "    \"How many chambers does the human heart have?\",\n",
        "    \"What is the deepest ocean trench?\",\n",
        "    \"Define artificial intelligence.\",\n",
        "    \"Explain machine learning.\"\n",
        "]\n",
        "\n",
        "# Define the ground truth answers for the evaluation queries\n",
        "ground_truth_answers = [\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"Quantum computing uses quantum-mechanical phenomena like superposition and entanglement.\",\n",
        "    \"The Amazon rainforest is the largest tropical rainforest in the world.\",\n",
        "    \"The human heart has four chambers: two atria and two ventricles.\",\n",
        "    \"The deepest ocean trench is the Mariana Trench, located in the western Pacific Ocean.\",\n",
        "    \"Artificial intelligence (AI) is intelligence demonstrated by machines.\",\n",
        "    \"Machine learning is a subset of AI that enables systems to learn from data.\"\n",
        "]\n",
        "\n",
        "# Initialize lists to store generated answers and retrieved contexts\n",
        "generated_answers = []\n",
        "retrieved_contexts_list = [] # List of lists, as each query might retrieve multiple documents\n",
        "\n",
        "print(\"Generating data for evaluation...\")\n",
        "\n",
        "# Run each query through the RAG system to collect generated answers and retrieved contexts\n",
        "for query in eval_queries:\n",
        "    # The agentic_rag_system function already handles retrieval and generation\n",
        "    # We need to modify it slightly or capture the intermediate outputs\n",
        "    # For simplicity in this example, let's re-run the retrieval and generation steps\n",
        "    # and capture the relevant information.\n",
        "\n",
        "    # --- Retrieval Step ---\n",
        "    retrieved_context = \"\"\n",
        "    try:\n",
        "        query_embedding_response = embedding_model.embed_content(model=EMBEDDING_MODEL_NAME, content=query)\n",
        "        if query_embedding_response and 'embedding' in query_embedding_response:\n",
        "            query_embedding = query_embedding_response['embedding']\n",
        "            relevant_docs = retrieve_relevant_documents(np.array(query_embedding))\n",
        "\n",
        "            if relevant_docs:\n",
        "                retrieved_contexts_list.append(relevant_docs) # Store as a list of documents\n",
        "                retrieved_context = \"\\n\".join(relevant_docs) # Join for the prompt\n",
        "            else:\n",
        "                retrieved_contexts_list.append([]) # Append empty list if no docs retrieved\n",
        "\n",
        "        else:\n",
        "            print(f\"Warning: Could not get embedding for query '{query[:50]}...', skipping retrieval.\")\n",
        "            retrieved_contexts_list.append([])\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during query embedding or retrieval for query '{query[:50]}...': {e}\")\n",
        "        retrieved_contexts_list.append([])\n",
        "\n",
        "\n",
        "    # --- Generation Step ---\n",
        "    prompt_parts = []\n",
        "    if retrieved_context:\n",
        "        prompt_parts.append(f\"Here is some relevant information:\\n{retrieved_context}\\n\\n\")\n",
        "    prompt_parts.append(f\"Based on the provided information (if any) and your general knowledge, please answer the following question:\\nQuestion: {query}\\n\\n\")\n",
        "    prompt_parts.append(\"Answer:\")\n",
        "\n",
        "    final_prompt = \"\".join(prompt_parts)\n",
        "\n",
        "    try:\n",
        "        response = generation_model.generate_content(final_prompt)\n",
        "        if response.candidates and response.candidates[0].content:\n",
        "            generated_answers.append(response.candidates[0].content.parts[0].text)\n",
        "        else:\n",
        "            generated_answers.append(\"Could not generate an answer.\")\n",
        "            if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
        "                 generated_answers[-1] += f\" Blocked reason: {response.prompt_feedback.block_reason.name}\"\n",
        "            elif response.candidates and response.candidates[0].finish_reason:\n",
        "                generated_answers[-1] += f\" Finish reason: {response.candidates[0].finish_reason.name}\"\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating content for query '{query[:50]}...': {e}\")\n",
        "        generated_answers.append(\"An error occurred during generation.\")\n",
        "\n",
        "\n",
        "# Create a pandas DataFrame for easier handling and compatibility with datasets\n",
        "import pandas as pd\n",
        "\n",
        "eval_data = pd.DataFrame({\n",
        "    'query': eval_queries,\n",
        "    'retrieved_context': retrieved_contexts_list, # Keep as list of strings for Ragas\n",
        "    'generated_answer': generated_answers,\n",
        "    'ground_truth_answer': ground_truth_answers\n",
        "})\n",
        "\n",
        "# Display the first few rows of the evaluation data\n",
        "print(\"\\n--- Sample Evaluation Data ---\")\n",
        "display(eval_data.head())\n",
        "\n",
        "# Convert the pandas DataFrame to a Hugging Face Dataset object, which is required by Ragas\n",
        "from datasets import Dataset\n",
        "\n",
        "eval_dataset = Dataset.from_pandas(eval_data)\n",
        "\n",
        "print(\"\\nEvaluation dataset created successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db00707d"
      },
      "source": [
        "## Calculate Ragas Metrics\n",
        "\n",
        "### Subtask:\n",
        "Use the Ragas library to compute the desired metrics (context relevancy, answer relevancy, and faithfulness) using the prepared evaluation data.\n",
        "\n",
        "**Reasoning**:\n",
        "The `ragas` library provides functions to calculate various metrics for evaluating RAG systems. We will use the `evaluate` function with the `context_relevancy`, `answer_relevancy`, and `faithfulness` metrics to assess the performance of our RAG system based on the prepared evaluation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a6df844"
      },
      "source": [
        "# --- 3. Calculate Ragas Metrics ---\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "print(\"Calculating Ragas metrics...\")\n",
        "\n",
        "# Define the metrics to be used\n",
        "metrics=[\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "    ]\n",
        "\n",
        "# Evaluate the dataset using the defined metrics\n",
        "try:\n",
        "    result = evaluate(\n",
        "        dataset=eval_dataset, # Use the dataset created in the previous step\n",
        "        metrics=metrics,\n",
        "        # You can add callbacks here if needed for logging or progress\n",
        "        # callbacks=[...],\n",
        "    )\n",
        "\n",
        "    print(\"\\nRagas Evaluation Results:\")\n",
        "    display(result)\n",
        "\n",
        "    # You can also convert the results to a pandas DataFrame for easier analysis\n",
        "    results_df = result.to_pandas()\n",
        "    print(\"\\nRagas Evaluation Results (DataFrame):\")\n",
        "    display(results_df)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Ragas evaluation: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}