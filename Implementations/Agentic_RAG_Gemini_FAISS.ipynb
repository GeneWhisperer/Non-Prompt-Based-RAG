{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7ab221f"
      },
      "source": [
        "import google.generativeai as genai\n",
        "import numpy as np\n",
        "# from sklearn.metrics.pairwise import cosine_similarity # No longer needed for retrieval with FAISS\n",
        "from IPython.display import Markdown, display\n",
        "import os\n",
        "import faiss # Import FAISS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79ae15ed"
      },
      "source": [
        "# --- 1. Configuration and API Key Setup ---\n",
        "try:\n",
        "    genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
        "    if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "        print(\"Warning: GOOGLE_API_KEY environment variable not set. Please set it for API access.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring Gemini API: {e}\")\n",
        "    raise # Re-raise to stop execution if models can't be initialized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b83dfd6"
      },
      "source": [
        "# --- 2. Initialize Models ---\n",
        "GENERATION_MODEL_NAME = 'gemini-1.5-flash-latest'\n",
        "EMBEDDING_MODEL_NAME = 'text-embedding-004'\n",
        "\n",
        "try:\n",
        "    generation_model = genai.GenerativeModel(GENERATION_MODEL_NAME)\n",
        "    embedding_model = genai.GenerativeModel(EMBEDDING_MODEL_NAME)\n",
        "    print(f\"Gemini models initialized: '{GENERATION_MODEL_NAME}' for generation, '{EMBEDDING_MODEL_NAME}' for embeddings.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Gemini models: {e}\")\n",
        "    raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56494797"
      },
      "source": [
        "# --- 3. Create a Simple In-Memory Knowledge Base ---\n",
        "knowledge_base_documents = [\n",
        "    \"The capital of France is Paris. Paris is known for its Eiffel Tower.\",\n",
        "    \"The Amazon rainforest is the largest tropical rainforest in the world.\",\n",
        "    \"Python is a high-level, general-purpose programming language.\",\n",
        "    \"Artificial intelligence (AI) is intelligence demonstrated by machines.\",\n",
        "    \"The human heart has four chambers: two atria and two ventricles.\",\n",
        "    \"The deepest ocean trench is the Mariana Trench, located in the western Pacific Ocean.\",\n",
        "    \"Machine learning is a subset of AI that enables systems to learn from data.\",\n",
        "    \"The Earth revolves around the Sun in an elliptical orbit.\",\n",
        "    \"Quantum computing uses quantum-mechanical phenomena like superposition and entanglement.\",\n",
        "    \"The Great Barrier Reef is the world's largest coral reef system, located off the coast of Queensland, Australia.\"\n",
        "]\n",
        "\n",
        "print(f\"Knowledge base loaded with {len(knowledge_base_documents)} documents.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d1512a0"
      },
      "source": [
        "# --- 4. Embed the Knowledge Base Documents and Build FAISS Index ---\n",
        "\n",
        "# Global variables to store the FAISS index and the mapping of index to document\n",
        "faiss_index = None\n",
        "document_store = [] # This will map the FAISS index to the original documents\n",
        "\n",
        "print(\"Starting document embedding process and FAISS index construction...\")\n",
        "embeddings_list = [] # Temporary list to collect all embeddings\n",
        "\n",
        "for i, doc in enumerate(knowledge_base_documents):\n",
        "    try:\n",
        "        embedding_response = embedding_model.embed_content(model=EMBEDDING_MODEL_NAME, content=doc)\n",
        "        if embedding_response and 'embedding' in embedding_response:\n",
        "            embedding = embedding_response['embedding']\n",
        "            embeddings_list.append(embedding)\n",
        "            document_store.append(doc) # Store the document at the same index\n",
        "            print(f\"Embedded document {i+1}/{len(knowledge_base_documents)}\")\n",
        "        else:\n",
        "            print(f\"Warning: No embedding found for document {i+1}: {doc[:50]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error embedding document {i+1} ('{doc[:50]}...'): {e}\")\n",
        "\n",
        "if not embeddings_list:\n",
        "    raise ValueError(\"No embeddings were successfully generated for the knowledge base. Cannot build FAISS index.\")\n",
        "\n",
        "# Convert list of embeddings to a NumPy array (float32 is standard for FAISS)\n",
        "embeddings_np = np.array(embeddings_list).astype('float32')\n",
        "\n",
        "# Get the dimensionality of the embeddings\n",
        "d = embeddings_np.shape[1]\n",
        "\n",
        "# Initialize a FAISS index. For simplicity, we use IndexFlatL2 for L2 distance (Euclidean).\n",
        "# For production, consider IndexIVFFlat for better performance with large datasets.\n",
        "try:\n",
        "    faiss_index = faiss.IndexFlatL2(d) # L2 distance is common for embeddings\n",
        "    faiss_index.add(embeddings_np) # Add the embeddings to the index\n",
        "    print(f\"FAISS index built with {faiss_index.ntotal} documents and dimensionality {d}.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error building FAISS index: {e}\")\n",
        "    raise # Stop if FAISS index cannot be built"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35b491dc"
      },
      "source": [
        "# --- 5. Retrieval Mechanism (Updated for FAISS) ---\n",
        "def retrieve_relevant_documents(query_embedding, top_k=2):\n",
        "    '''\n",
        "    Retrieves the top_k most relevant documents from the FAISS index\n",
        "    based on similarity with the query embedding.\n",
        "\n",
        "    Args:\n",
        "        query_embedding (np.array): The embedding of the user's query.\n",
        "        top_k (int): The number of top documents to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of relevant documents (strings).\n",
        "    '''\n",
        "    global faiss_index, document_store\n",
        "\n",
        "    if faiss_index is None or faiss_index.ntotal == 0:\n",
        "        print(\"Warning: FAISS index is not initialized or empty, no documents to retrieve.\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        # Reshape query_embedding to be a 2D array (1, d) as required by FAISS search\n",
        "        query_embedding_faiss = query_embedding.reshape(1, -1).astype('float32')\n",
        "\n",
        "        # Perform the search: D are distances, I are indices\n",
        "        # FAISS returns distances (D) and indices (I) of the top_k nearest neighbors\n",
        "        D, I = faiss_index.search(query_embedding_faiss, top_k)\n",
        "\n",
        "        retrieved_docs = []\n",
        "        for idx in I[0]: # I[0] contains the indices for the first (and only) query\n",
        "            if idx != -1: # FAISS returns -1 for empty slots if k > ntotal\n",
        "                retrieved_docs.append(document_store[idx]) # Retrieve the actual document\n",
        "        print(f\"Retrieved {len(retrieved_docs)} documents using FAISS.\")\n",
        "        return retrieved_docs\n",
        "    except Exception as e:\n",
        "        print(f\"Error during FAISS retrieval: {e}\")\n",
        "        return []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78963038"
      },
      "source": [
        "# --- 6. Agentic Logic for RAG (No changes here, as it calls the updated retrieval function) ---\n",
        "def agentic_rag_system(user_query):\n",
        "    '''\n",
        "    An agentic RAG system that decides whether to retrieve information\n",
        "    before generating a response.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's input query.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response from the RAG system.\n",
        "    '''\n",
        "    print(f\"\\n--- User Query: '{user_query}' ---\")\n",
        "\n",
        "    # --- Agent's Decision Phase ---\n",
        "    keywords_for_retrieval = [\"what is\", \"tell me about\", \"where is\", \"who is\", \"explain\", \"describe\", \"facts about\", \"information on\"]\n",
        "    needs_retrieval = any(keyword in user_query.lower() for keyword in keywords_for_retrieval)\n",
        "\n",
        "    retrieved_context = \"\"\n",
        "    if needs_retrieval:\n",
        "        print(\"Agent decision: Retrieval is likely needed based on keywords.\")\n",
        "        try:\n",
        "            query_embedding_response = embedding_model.embed_content(model=EMBEDDING_MODEL_NAME, content=user_query)\n",
        "            if query_embedding_response and 'embedding' in query_embedding_response:\n",
        "                query_embedding = query_embedding_response['embedding']\n",
        "                relevant_docs = retrieve_relevant_documents(np.array(query_embedding))\n",
        "\n",
        "                if relevant_docs:\n",
        "                    retrieved_context = \"\\n\".join(relevant_docs)\n",
        "                    print(\"\\n--- Retrieved Context ---\")\n",
        "                    display(Markdown(f\"```text\\n{retrieved_context}\\n```\"))\n",
        "                else:\n",
        "                    print(\"No relevant documents found for retrieval.\")\n",
        "            else:\n",
        "                print(\"Warning: Could not get embedding for the user query, skipping retrieval.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during query embedding or retrieval: {e}\")\n",
        "            retrieved_context = \"\"\n",
        "    else:\n",
        "        print(\"Agent decision: Direct generation without retrieval (no relevant keywords detected).\")\n",
        "\n",
        "    # --- Generation Phase ---\n",
        "    prompt_parts = []\n",
        "    if retrieved_context:\n",
        "        prompt_parts.append(f\"Here is some relevant information:\\n{retrieved_context}\\n\\n\")\n",
        "    prompt_parts.append(f\"Based on the provided information (if any) and your general knowledge, please answer the following question:\\nQuestion: {user_query}\\n\\n\")\n",
        "    prompt_parts.append(\"Answer:\")\n",
        "\n",
        "    final_prompt = \"\".join(prompt_parts)\n",
        "    print(f\"\\n--- Final Prompt Sent to Gemini ---\\n```\\n{final_prompt}\\n```\")\n",
        "\n",
        "    try:\n",
        "        response = generation_model.generate_content(final_prompt)\n",
        "\n",
        "        if response.candidates and response.candidates[0].content:\n",
        "            generated_text = response.candidates[0].content.parts[0].text\n",
        "            print(\"\\n--- Generated Response ---\")\n",
        "            display(Markdown(generated_text))\n",
        "        else:\n",
        "            print(\"\\n--- No Valid Response from Gemini ---\")\n",
        "            generated_text = \"I couldn't generate a valid response for that query.\"\n",
        "            if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
        "                print(f\"Blocked reason: {response.prompt_feedback.block_reason.name}\")\n",
        "                generated_text += f\"\\nReason: {response.prompt_feedback.block_reason.name}\"\n",
        "            elif response.candidates and response.candidates[0].finish_reason:\n",
        "                print(f\"Finish reason: {response.candidates[0].finish_reason.name}\")\n",
        "                generated_text += f\"\\nFinish Reason: {response.candidates[0].finish_reason.name}\"\n",
        "            display(Markdown(generated_text))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- Error Generating Content with Gemini ---\")\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        generated_text = \"An error occurred while generating the response.\"\n",
        "        display(Markdown(generated_text))\n",
        "\n",
        "    return generated_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef4dd68e"
      },
      "source": [
        "# --- Example Usage ---\n",
        "print(\"\\n--- Running Example Queries ---\")\n",
        "agentic_rag_system(\"What is the capital of France?\")\n",
        "agentic_rag_system(\"Tell me about quantum computing.\")\n",
        "agentic_rag_system(\"What is the largest tropical rainforest?\")\n",
        "agentic_rag_system(\"How many chambers does the human heart have?\")\n",
        "agentic_rag_system(\"What is the deepest ocean trench?\")\n",
        "agentic_rag_system(\"Tell me about the universe.\")\n",
        "agentic_rag_system(\"Define artificial intelligence.\")\n",
        "agentic_rag_system(\"Explain machine learning.\")\n",
        "agentic_rag_system(\"Tell me about the Big Bang theory.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}