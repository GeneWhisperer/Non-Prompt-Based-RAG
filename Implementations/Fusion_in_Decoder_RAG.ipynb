{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ragas"
      ],
      "metadata": {
        "id": "h5YqZU5bs8jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eb1f1ad"
      },
      "source": [
        "import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM # No longer needed for generation\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import Dataset\n",
        "import faiss\n",
        "import numpy as np\n",
        "import textwrap\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "from typing import List, Dict, Any"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b530590"
      },
      "source": [
        "# --- Import Google Generative AI Library ---\n",
        "import google.generativeai as genai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ae33a24"
      },
      "source": [
        "# --- Configuration Management ---\n",
        "class Config:\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Retriever Config\n",
        "    EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "    EMBEDDING_BATCH_SIZE = 32\n",
        "    RETRIEVAL_TOP_K = 3\n",
        "\n",
        "    # Generator Config (Now for Gemini API)\n",
        "    GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\") # IMPORTANT: Store securely, e.g., environment variable\n",
        "    GEMINI_MODEL_NAME = \"gemini-pro\" # Or \"gemini-1.5-pro\", \"gemini-1.0-pro\", etc. based on availability\n",
        "    # GENERATOR_MAX_LENGTH and GENERATOR_NUM_BEAMS are not directly applicable to Gemini API calls in the same way,\n",
        "    # but you can control response length and safety settings via API parameters.\n",
        "    GENERATOR_MAX_LENGTH = 256 # Example: Set a default max length for Gemini output\n",
        "\n",
        "    # Logging Config\n",
        "    LOG_LEVEL = logging.INFO\n",
        "    LOG_FILE = \"rag_pipeline.log\"\n",
        "\n",
        "    # Data Paths\n",
        "    KNOWLEDGE_BASE_PATH = \"data/documents.txt\"\n",
        "\n",
        "    # FAISS Config\n",
        "    FAISS_INDEX_TYPE = \"IndexFlatL2\" # Add FAISS index type to config\n",
        "\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(level=Config.LOG_LEVEL,\n",
        "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "                    handlers=[\n",
        "                        logging.FileHandler(Config.LOG_FILE),\n",
        "                        logging.StreamHandler()\n",
        "                    ])\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa024d9a"
      },
      "source": [
        "# --- 0. Environment Setup (Local dependencies for Retriever) ---\n",
        "def setup_environment():\n",
        "    try:\n",
        "        # import transformers # Removed as generator model is external\n",
        "        import datasets\n",
        "        # import accelerate # Accelerate is more for local training/inference optimization\n",
        "        import faiss\n",
        "        import sentence_transformers\n",
        "        import google.generativeai # Check for gemini library\n",
        "        logger.info(\"All required libraries are already installed.\")\n",
        "    except ImportError:\n",
        "        logger.warning(\"Required libraries not found. Installing them now...\")\n",
        "        # Note: 'accelerate' is not strictly needed if you're not using local HF models with it.\n",
        "        os.system(\"pip install datasets faiss-cpu sentence-transformers google-generativeai\")\n",
        "        logger.info(\"Libraries installed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e289de2d"
      },
      "source": [
        "# --- 1. Data Preparation ---\n",
        "class KnowledgeBase:\n",
        "    def __init__(self, documents: List[str]):\n",
        "        self.documents = documents\n",
        "        self.corpus_dataset = Dataset.from_dict({\"text\": documents})\n",
        "        logger.info(f\"Corpus size: {len(self.corpus_dataset)} documents\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_text_list(cls, text_list: List[str]):\n",
        "        return cls(text_list)\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, file_path: str):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                documents = [line.strip() for line in f if line.strip()]\n",
        "            logger.info(f\"Loaded {len(documents)} documents from {file_path}\")\n",
        "            return cls(documents)\n",
        "        except FileNotFoundError:\n",
        "            logger.error(f\"Knowledge base file not found: {file_path}\")\n",
        "            return cls([])\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading knowledge base from file: {e}\")\n",
        "            return cls([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ad95fca"
      },
      "source": [
        "# --- 2. Retriever Module (Remains largely the same) ---\n",
        "class Retriever:\n",
        "    def __init__(self, config: Config, knowledge_base: KnowledgeBase):\n",
        "        self.config = config\n",
        "        self.knowledge_base = knowledge_base\n",
        "        self.device = self.config.DEVICE\n",
        "        self.embedding_model = self._load_embedding_model()\n",
        "        self.index = self._build_faiss_index()\n",
        "\n",
        "    def _load_embedding_model(self):\n",
        "        logger.info(f\"Loading embedding model: {self.config.EMBEDDING_MODEL_NAME}...\")\n",
        "        try:\n",
        "            model = SentenceTransformer(self.config.EMBEDDING_MODEL_NAME).to(self.device)\n",
        "            logger.info(\"Embedding model loaded successfully.\")\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            logger.critical(f\"Failed to load embedding model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _build_faiss_index(self):\n",
        "        logger.info(\"Generating document embeddings...\")\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            document_embeddings = self.embedding_model.encode(\n",
        "                self.knowledge_base.corpus_dataset[\"text\"],\n",
        "                batch_size=self.config.EMBEDDING_BATCH_SIZE,\n",
        "                convert_to_tensor=True,\n",
        "                show_progress_bar=False, # Set to False for cleaner logs in non-interactive\n",
        "                device=self.device\n",
        "            )\n",
        "            document_embeddings_np = document_embeddings.cpu().numpy()\n",
        "            logger.info(f\"Document embeddings shape: {document_embeddings_np.shape}\")\n",
        "            logger.info(f\"Embedding generation time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "            dimension = document_embeddings_np.shape[1]\n",
        "            if self.config.FAISS_INDEX_TYPE == \"IndexFlatL2\":\n",
        "                index = faiss.IndexFlatL2(dimension)\n",
        "            else:\n",
        "                logger.warning(f\"Unsupported FAISS index type: {self.config.FAISS_INDEX_TYPE}. Falling back to IndexFlatL2.\")\n",
        "                index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "\n",
        "            index.add(document_embeddings_np)\n",
        "            logger.info(f\"FAISS index built with {index.ntotal} vectors using {self.config.FAISS_INDEX_TYPE}.\")\n",
        "            return index\n",
        "        except Exception as e:\n",
        "            logger.critical(f\"Failed to build FAISS index: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def retrieve_documents(self, query: str, k: int = None) -> List[str]:\n",
        "        if k is None:\n",
        "            k = self.config.RETRIEVAL_TOP_K\n",
        "\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            query_embedding = self.embedding_model.encode(query, convert_to_tensor=True, device=self.device)\n",
        "            query_embedding_np = query_embedding.cpu().numpy().reshape(1, -1)\n",
        "\n",
        "            distances, indices = self.index.search(query_embedding_np, k)\n",
        "\n",
        "            retrieved_docs = [self.knowledge_base.corpus_dataset[\"text\"][idx] for idx in indices[0]]\n",
        "            logger.info(f\"Retrieved {len(retrieved_docs)} documents in {time.time() - start_time:.4f} seconds for query: '{query[:50]}...'\")\n",
        "            return retrieved_docs\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during document retrieval for query '{query[:50]}...': {e}\")\n",
        "            return []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f8625f5"
      },
      "source": [
        "# --- 3. Generator Module (Re-written for Gemini API) ---\n",
        "class Generator:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.model = self._load_gemini_model()\n",
        "\n",
        "    def _load_gemini_model(self):\n",
        "        logger.info(f\"Configuring Gemini API with model: {self.config.GEMINI_MODEL_NAME}...\")\n",
        "        try:\n",
        "            if not self.config.GEMINI_API_KEY:\n",
        "                raise ValueError(\"GEMINI_API_KEY is not set. Please set it as an environment variable.\")\n",
        "            genai.configure(api_key=self.config.GEMINI_API_KEY)\n",
        "            model = genai.GenerativeModel(self.config.GEMINI_MODEL_NAME)\n",
        "            logger.info(\"Gemini model configured successfully.\")\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            logger.critical(f\"Failed to configure Gemini model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def generate_answer(self, query: str, retrieved_docs: List[str]) -> str:\n",
        "        start_time = time.time()\n",
        "\n",
        "        if not retrieved_docs:\n",
        "            logger.warning(f\"No documents retrieved for query: '{query[:50]}...'. Generating answer without context.\")\n",
        "            prompt = f\"Question: {query}\\nAnswer:\"\n",
        "        else:\n",
        "            context = \"\\n\".join([f\"- {doc}\" for doc in retrieved_docs]) # Format context nicely\n",
        "            prompt = f\"\"\"Based on the following context, answer the question. If the information is not in the context, state that you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Call Gemini API\n",
        "            # You can add generation_config and safety_settings here if needed\n",
        "            response = self.model.generate_content(\n",
        "                prompt,\n",
        "                generation_config=genai.types.GenerationConfig(\n",
        "                    temperature=0.2,  # Example: lower temperature for more deterministic answers\n",
        "                    max_output_tokens=self.config.GENERATOR_MAX_LENGTH # This maps roughly to max_length\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Extract text from the response\n",
        "            answer = response.text\n",
        "            logger.info(f\"Generated answer in {time.time() - start_time:.4f} seconds for query: '{query[:50]}...'\")\n",
        "            return answer\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during Gemini API call for query '{query[:50]}...': {e}\")\n",
        "            return \"An error occurred while generating the answer using the Gemini API.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47e09196"
      },
      "source": [
        "# --- 4. RAG Pipeline Orchestrator (Remains largely the same) ---\n",
        "class RAGPipeline:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "        documents = [\n",
        "            \"The capital of France is Paris.\",\n",
        "            \"Eiffel Tower is located in Paris, France.\",\n",
        "            \"The Louvre Museum is a famous art museum in Paris.\",\n",
        "            \"Germany is a country in Central Europe.\",\n",
        "            \"Berlin is the capital and largest city of Germany.\",\n",
        "            \"The Brandenburg Gate is an 18th-century neoclassical monument in Berlin.\",\n",
        "            \"Artificial intelligence (AI) is a rapidly advancing field of computer science.\",\n",
        "            \"Machine learning is a subset of AI that focuses on algorithms that learn from data.\",\n",
        "            \"Deep learning is a subset of machine learning using neural networks with many layers.\",\n",
        "            \"Python is a popular programming language for AI and machine learning.\"\n",
        "        ]\n",
        "        self.knowledge_base = KnowledgeBase.from_text_list(documents)\n",
        "\n",
        "        self.retriever = Retriever(self.config, self.knowledge_base)\n",
        "        self.generator = Generator(self.config)\n",
        "\n",
        "    def query(self, query_text: str) -> Dict[str, Any]:\n",
        "        logger.info(f\"Processing query: '{query_text}'\")\n",
        "        pipeline_start_time = time.time()\n",
        "\n",
        "        retrieval_start_time = time.time()\n",
        "        retrieved_docs = self.retriever.retrieve_documents(query_text)\n",
        "        retrieval_end_time = time.time()\n",
        "        logger.info(f\"Retrieval took: {retrieval_end_time - retrieval_start_time:.4f} seconds\")\n",
        "\n",
        "        generation_start_time = time.time()\n",
        "        answer = self.generator.generate_answer(query_text, retrieved_docs)\n",
        "        generation_end_time = time.time()\n",
        "        logger.info(f\"Generation took: {generation_end_time - generation_start_time:.4f} seconds\")\n",
        "\n",
        "\n",
        "        pipeline_end_time = time.time()\n",
        "        logger.info(f\"Total pipeline time: {pipeline_end_time - pipeline_start_time:.4f} seconds\")\n",
        "\n",
        "        return {\n",
        "            \"query\": query_text,\n",
        "            \"retrieved_documents\": retrieved_docs,\n",
        "            \"answer\": answer,\n",
        "            \"metrics\": {\n",
        "                \"retrieval_time_sec\": retrieval_end_time - retrieval_start_time,\n",
        "                \"generation_time_sec\": generation_end_time - generation_start_time,\n",
        "                \"total_pipeline_time_sec\": pipeline_end_time - pipeline_start_time,\n",
        "                \"num_retrieved_docs\": len(retrieved_docs)\n",
        "            }\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1df0129a"
      },
      "source": [
        "# --- 5. Test the RAG System ---\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting RAG system initialization...\")\n",
        "    try:\n",
        "        setup_environment() # Call this to ensure google-generativeai is installed\n",
        "\n",
        "        config = Config()\n",
        "\n",
        "        # Ensure GEMINI_API_KEY is set before proceeding\n",
        "        if not config.GEMINI_API_KEY:\n",
        "            logger.critical(\"GEMINI_API_KEY environment variable not set. Please set it to run the Gemini-powered generator.\")\n",
        "            exit(1) # Exit if API key is missing\n",
        "\n",
        "\n",
        "        rag_pipeline = RAGPipeline(config)\n",
        "        logger.info(\"RAG system initialized successfully.\")\n",
        "\n",
        "        print(\"\\n--- Testing RAG System ---\")\n",
        "\n",
        "        queries = [\n",
        "            \"What is the capital of France?\",\n",
        "            \"Tell me about monuments in Berlin.\",\n",
        "            \"What is deep learning?\",\n",
        "            \"Which programming language is good for AI?\",\n",
        "            \"Where is the Eiffel Tower?\",\n",
        "            \"Who won the World Series in 2023?\" # Example of out-of-domain query\n",
        "        ]\n",
        "\n",
        "        # Store results for Ragas evaluation\n",
        "        query_results = []\n",
        "\n",
        "        for i, query_text in enumerate(queries):\n",
        "            print(f\"\\nQuery {i+1}: {query_text}\")\n",
        "\n",
        "            result = rag_pipeline.query(query_text)\n",
        "            query_results.append(result) # Add result to list for Ragas\n",
        "\n",
        "            print(\"Retrieved Documents:\")\n",
        "            if result[\"retrieved_documents\"]:\n",
        "                for j, doc in enumerate(result[\"retrieved_documents\"]):\n",
        "                    print(f\"  {j+1}. {textwrap.fill(doc, width=80)}\")\n",
        "            else:\n",
        "                print(\"  No relevant documents retrieved.\")\n",
        "\n",
        "            print(f\"Generated Answer: {textwrap.fill(result['answer'], width=80)}\")\n",
        "            print(f\"Metrics: {result['metrics']}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        # --- Ragas Evaluation ---\n",
        "        print(\"\\n--- Ragas Evaluation ---\")\n",
        "        ragas_dataset = prepare_ragas_dataset(query_results)\n",
        "        ragas_scores = evaluate_ragas_metrics(ragas_dataset)\n",
        "\n",
        "        print(\"\\nRagas Scores:\")\n",
        "        display(ragas_scores)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"RAG system encountered a critical error during initialization or testing: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "437e8d7a"
      },
      "source": [
        "!pip install -q ragas datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecdc7f2c"
      },
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def prepare_ragas_dataset(query_results: List[Dict[str, Any]]) -> Dataset:\n",
        "    \"\"\"\n",
        "    Prepares a Dataset object for Ragas evaluation from the RAG pipeline results.\n",
        "\n",
        "    Args:\n",
        "        query_results: A list of dictionaries, where each dictionary is the output\n",
        "                       of the RAGPipeline.query() method.\n",
        "\n",
        "    Returns:\n",
        "        A Dataset object compatible with Ragas.\n",
        "    \"\"\"\n",
        "    questions = [result[\"query\"] for result in query_results]\n",
        "    answers = [result[\"answer\"] for result in query_results]\n",
        "    contexts = [result[\"retrieved_documents\"] for result in query_results]\n",
        "\n",
        "    # Note: Ragas metrics like faithfulness and answer relevancy ideally require\n",
        "    # 'ground_truths'. Since we don't have a predefined dataset with ground truths\n",
        "    # in this example, we will omit them. Some metrics like context relevancy\n",
        "    # can still be calculated without ground_truths.\n",
        "    # If you have ground truths, add them like this:\n",
        "    # ground_truths = [result[\"ground_truth\"] for result in query_results]\n",
        "    # data = {'question': questions, 'answer': answers, 'contexts': contexts, 'ground_truths': ground_truths}\n",
        "\n",
        "    data = {'question': questions, 'answer': answers, 'contexts': contexts}\n",
        "\n",
        "    return Dataset.from_dict(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b62a23d"
      },
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    answer_relevancy,\n",
        "    context_relevancy,\n",
        "    faithfulness,\n",
        ")\n",
        "\n",
        "def evaluate_ragas_metrics(ragas_dataset: Dataset):\n",
        "    \"\"\"\n",
        "    Evaluates Ragas metrics on a given dataset.\n",
        "\n",
        "    Args:\n",
        "        ragas_dataset: A Dataset object compatible with Ragas.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the calculated Ragas scores.\n",
        "    \"\"\"\n",
        "    logger.info(\"Calculating Ragas metrics...\")\n",
        "    metrics = [\n",
        "        context_relevancy,\n",
        "        answer_relevancy,\n",
        "        faithfulness,\n",
        "        # Add other metrics here if needed\n",
        "    ]\n",
        "\n",
        "    # Note: Faithfulness and Answer Relevancy require a generator model.\n",
        "    # Ragas can use various models, including OpenAI and Hugging Face.\n",
        "    # For this example, we'll assume Ragas can infer a suitable model or\n",
        "    # you might need to configure it based on your Ragas setup.\n",
        "    # If you need to specify a model, you might do it like this:\n",
        "    # from ragas.llms import OpenAI\n",
        "    # openai_model = OpenAI(api_key=\"YOUR_OPENAI_API_KEY\")\n",
        "    # result = evaluate(ragas_dataset, metrics=metrics, llm=openai_model)\n",
        "    # However, Ragas often works out-of-the-box with default models if available.\n",
        "\n",
        "    result = evaluate(ragas_dataset, metrics=metrics)\n",
        "\n",
        "\n",
        "    logger.info(\"Ragas evaluation complete.\")\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}