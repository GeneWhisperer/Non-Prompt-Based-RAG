{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8391987",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "d8391987"
      },
      "outputs": [],
      "source": [
        "!pip install -q numpy langchain langchain-google-genai google-generativeai faiss-cpu rank_bm25 ragas datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "mZDC8xWKvD2G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from langchain.embeddings import GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from rank_bm25 import BM25Okapi\n",
        "from typing import List, Tuple\n",
        "\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_relevancy,\n",
        ")\n",
        "from datasets import Dataset"
      ],
      "id": "mZDC8xWKvD2G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4251c4f8",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "4251c4f8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from langchain.embeddings import GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from rank_bm25 import BM25Okapi\n",
        "from typing import List, Tuple\n",
        "\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_relevancy,\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "class GeminiRAG:\n",
        "    def __init__(self, documents: List[str], embedding_model=None, knoweledge_base=None):\n",
        "        self.documents = documents\n",
        "        # Use Gemini embedding model via LangChain (GoogleGenerativeAIEmbeddings)\n",
        "        self.embedding_model = embedding_model or GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        # Semantic Index (FAISS)\n",
        "        self.faiss_index = FAISS.from_texts(documents, self.embedding_model)\n",
        "        # BM25 Index\n",
        "        self.bm25 = BM25Okapi([doc.split() for doc in documents])\n",
        "        \"\"\"\n",
        "        Initialize the GeminiRAG instance.\n",
        "\n",
        "        Args:\n",
        "            knowledge_base: Optional knowledge base object to perform RAG.\n",
        "        \"\"\"\n",
        "        self.knowledge_base = knoweledge_base\n",
        "\n",
        "    def semantic_search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "        docs_and_scores = self.faiss_index.similarity_search_with_score(query, k=top_k)\n",
        "        return [(doc.page_content, score) for doc, score in docs_and_scores]\n",
        "\n",
        "    def bm25_search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "        scores = self.bm25.get_scores(query.split())\n",
        "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "        return [(self.documents[i], scores[i]) for i in top_indices]\n",
        "\n",
        "    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.5) -> List[Tuple[str, float]]:\n",
        "        sem_results = dict(self.semantic_search(query, top_k=top_k*2))\n",
        "        bm25_results = dict(self.bm25_search(query, top_k=top_k*2))\n",
        "        sem_max = max(sem_results.values(), default=1)\n",
        "        bm25_max = max(bm25_results.values(), default=1)\n",
        "        sem_results_norm = {k: v / sem_max for k, v in sem_results.items()}\n",
        "        bm25_results_norm = {k: v / bm25_max for k, v in bm25_results.items()}\n",
        "        all_docs = set(sem_results) | set(bm25_results)\n",
        "        hybrid_scores = {\n",
        "            doc: alpha * sem_results_norm.get(doc, 0) + (1 - alpha) * bm25_results_norm.get(doc, 0)\n",
        "            for doc in all_docs\n",
        "        }\n",
        "        ranked = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "        return ranked\n",
        "\n",
        "    def hyde(self, question: str, model=None) -> str:\n",
        "        \"\"\"\n",
        "        Generate a hypothetical answer (HyDE) to the question to use as a retrieval query.\n",
        "        Uses Gemini via LangChain.\n",
        "        \"\"\"\n",
        "        # Use LangChain's Gemini LLM wrapper (GoogleGenerativeAI)\n",
        "        from langchain.llms import GoogleGenerativeAI\n",
        "        llm = model or GoogleGenerativeAI(model=\"models/gemini-pro\", temperature=0)\n",
        "        hypothetical_answer = llm(question)\n",
        "        return hypothetical_answer\n",
        "\n",
        "    def hyde_hybrid_search(self, question: str, top_k: int = 5, alpha: float = 0.5, model=None) -> List[Tuple[str, float]]:\n",
        "        hypothetical_answer = self.hyde(question, model=model)\n",
        "        return self.hybrid_search(hypothetical_answer, top_k=top_k, alpha=alpha)\n",
        "\n",
        "    def chat(self, prompt: str) -> Tuple[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Generate a response to the given prompt using RAG.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): The user input prompt.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[str, List[str]]: The generated response and the retrieved context.\n",
        "        \"\"\"\n",
        "        # Example implementation (replace with actual logic)\n",
        "        context = []\n",
        "        if self.knowledge_base:\n",
        "            context = self.knowledge_base.retrieve(prompt)\n",
        "            # Replace with actual LLM call using context\n",
        "            response = f\"Context: {context}\\nResponse: This is a generated answer based on the context.\"\n",
        "        else:\n",
        "             # Replace with actual LLM call without context\n",
        "             response = \"Response: This is a generated answer without context.\"\n",
        "\n",
        "        return response, context\n",
        "\n",
        "    def evaluate_ragas(self, query: str, answer: str, context: List[str]):\n",
        "        \"\"\"\n",
        "        Evaluate RAG metrics for a given query, answer, and context.\n",
        "\n",
        "        Args:\n",
        "            query (str): The user query.\n",
        "            answer (str): The generated answer.\n",
        "            context (List[str]): The retrieved context.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the Ragas metrics.\n",
        "        \"\"\"\n",
        "        data = {\n",
        "            \"question\": [query],\n",
        "            \"answer\": [answer],\n",
        "            \"contexts\": [context]\n",
        "        }\n",
        "        dataset = Dataset.from_dict(data)\n",
        "\n",
        "        result = evaluate(\n",
        "            dataset,\n",
        "            metrics=[\n",
        "                faithfulness,\n",
        "                answer_relevancy,\n",
        "                context_relevancy,\n",
        "            ],\n",
        "        )\n",
        "        return result.to_pandas().iloc[0].to_dict()\n",
        "\n",
        "    def evaluate_dataset(self, dataset: Dataset):\n",
        "        \"\"\"\n",
        "        Evaluate RAG metrics for a dataset of queries, answers, and contexts.\n",
        "\n",
        "        Args:\n",
        "            dataset (Dataset): The dataset containing queries, answers, and contexts.\n",
        "\n",
        "        Returns:\n",
        "            pandas.DataFrame: A DataFrame containing the Ragas metrics for each sample in the dataset.\n",
        "        \"\"\"\n",
        "        return evaluate(\n",
        "            dataset,\n",
        "            metrics=[\n",
        "                faithfulness,\n",
        "                answer_relevancy,\n",
        "                context_relevancy,\n",
        "            ],\n",
        "        ).to_pandas()"
      ]
    },
    {
      "source": [
        "# Assuming you have a GeminiRAG instance named rag_system\n",
        "# And you have a query, generated answer, and retrieved context\n",
        "\n",
        "query = \"What is the capital of France?\"\n",
        "answer = \"Paris is the capital of France.\"\n",
        "context = [\"France is a country in Europe. Its capital is Paris.\"]\n",
        "\n",
        "# Evaluate a single sample\n",
        "ragas_metrics = rag_system.evaluate_ragas(query, answer, context)\n",
        "print(ragas_metrics)\n",
        "\n",
        "# Assuming you have a dataset in the format of a datasets.Dataset object\n",
        "# with columns \"question\", \"answer\", and \"contexts\"\n",
        "\n",
        "# Evaluate a dataset\n",
        "dataset_ragas_metrics = rag_system.evaluate_dataset(dataset)\n",
        "print(dataset_ragas_metrics)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vBNOfjvDv4Kw"
      },
      "id": "vBNOfjvDv4Kw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}