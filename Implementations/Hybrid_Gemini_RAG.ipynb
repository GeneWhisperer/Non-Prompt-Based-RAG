{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8391987",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "d8391987"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-google-genai google-generativeai faiss-cpu rank_bm25 ragas datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q numpy langchain google-generativeai faiss-cpu rank_bm25 ragas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejA8o3B75WL5",
        "outputId": "23949279-025c-4992-9600-d87b5cbef8cc"
      },
      "id": "ejA8o3B75WL5",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/190.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "mZDC8xWKvD2G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# from langchain.embeddings import GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from rank_bm25 import BM25Okapi\n",
        "from typing import List, Tuple\n",
        "\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "from datasets import Dataset"
      ],
      "id": "mZDC8xWKvD2G"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -Uq sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGzkhCUr61RG",
        "outputId": "b96cf048-75fe-4857-de7e-32ade6d08df1"
      },
      "id": "KGzkhCUr61RG",
      "execution_count": 12,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m470.2/470.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "sT5TVA_r8DwH"
      },
      "id": "sT5TVA_r8DwH",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7Xygh9S8H5M"
      },
      "id": "G7Xygh9S8H5M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cn2e_QnQ8X1I"
      },
      "id": "Cn2e_QnQ8X1I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xRYY2q7c8Xfl"
      },
      "id": "xRYY2q7c8Xfl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######not working"
      ],
      "metadata": {
        "id": "P4sji5iq-C17"
      },
      "id": "P4sji5iq-C17"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4251c4f8",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "4251c4f8"
      },
      "outputs": [],
      "source": [
        "class GeminiRAG:\n",
        "    def __init__(self, documents: List[str], embedding_model=None, knoweledge_base=None):\n",
        "        self.documents = documents\n",
        "        # Use Gemini embedding model via LangChain (GoogleGenerativeAIEmbeddings)\n",
        "        self.embedding_model = sentence-transformers/all-MiniLM-L6-v2\n",
        "        # Semantic Index (FAISS)\n",
        "        self.faiss_index = FAISS.from_texts(documents, self.embedding_model)\n",
        "        # BM25 Index\n",
        "        self.bm25 = BM25Okapi([doc.split() for doc in documents])\n",
        "        \"\"\"\n",
        "        Initialize the GeminiRAG instance.\n",
        "\n",
        "        Args:\n",
        "            knowledge_base: Optional knowledge base object to perform RAG.\n",
        "        \"\"\"\n",
        "        self.knowledge_base = knoweledge_base\n",
        "\n",
        "    def semantic_search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "        docs_and_scores = self.faiss_index.similarity_search_with_score(query, k=top_k)\n",
        "        return [(doc.page_content, score) for doc, score in docs_and_scores]\n",
        "\n",
        "    def bm25_search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "        scores = self.bm25.get_scores(query.split())\n",
        "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "        return [(self.documents[i], scores[i]) for i in top_indices]\n",
        "\n",
        "    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.5) -> List[Tuple[str, float]]:\n",
        "        sem_results = dict(self.semantic_search(query, top_k=top_k*2))\n",
        "        bm25_results = dict(self.bm25_search(query, top_k=top_k*2))\n",
        "        sem_max = max(sem_results.values(), default=1)\n",
        "        bm25_max = max(bm25_results.values(), default=1)\n",
        "        sem_results_norm = {k: v / sem_max for k, v in sem_results.items()}\n",
        "        bm25_results_norm = {k: v / bm25_max for k, v in bm25_results.items()}\n",
        "        all_docs = set(sem_results) | set(bm25_results)\n",
        "        hybrid_scores = {\n",
        "            doc: alpha * sem_results_norm.get(doc, 0) + (1 - alpha) * bm25_results_norm.get(doc, 0)\n",
        "            for doc in all_docs\n",
        "        }\n",
        "        ranked = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "        return ranked\n",
        "\n",
        "    def hyde(self, question: str, model=None) -> str:\n",
        "        \"\"\"\n",
        "        Generate a hypothetical answer (HyDE) to the question to use as a retrieval query.\n",
        "        Uses Gemini via LangChain.\n",
        "        \"\"\"\n",
        "        # Use LangChain's Gemini LLM wrapper (GoogleGenerativeAI)\n",
        "        from langchain.llms import GoogleGenerativeAI\n",
        "        llm = model or GoogleGenerativeAI(model=\"models/gemini-pro\", temperature=0)\n",
        "        hypothetical_answer = llm(question)\n",
        "        return hypothetical_answer\n",
        "\n",
        "    def hyde_hybrid_search(self, question: str, top_k: int = 5, alpha: float = 0.5, model=None) -> List[Tuple[str, float]]:\n",
        "        hypothetical_answer = self.hyde(question, model=model)\n",
        "        return self.hybrid_search(hypothetical_answer, top_k=top_k, alpha=alpha)\n",
        "\n",
        "    def chat(self, prompt: str) -> Tuple[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Generate a response to the given prompt using RAG.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): The user input prompt.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[str, List[str]]: The generated response and the retrieved context.\n",
        "        \"\"\"\n",
        "        # Example implementation (replace with actual logic)\n",
        "        context = []\n",
        "        if self.knowledge_base:\n",
        "            context = self.knowledge_base.retrieve(prompt)\n",
        "            # Replace with actual LLM call using context\n",
        "            response = f\"Context: {context}\\nResponse: This is a generated answer based on the context.\"\n",
        "        else:\n",
        "             # Replace with actual LLM call without context\n",
        "             response = \"Response: This is a generated answer without context.\"\n",
        "\n",
        "        return response, context\n",
        "\n",
        "    def evaluate_ragas(self, query: str, answer: str, context: List[str]):\n",
        "        \"\"\"\n",
        "        Evaluate RAG metrics for a given query, answer, and context.\n",
        "\n",
        "        Args:\n",
        "            query (str): The user query.\n",
        "            answer (str): The generated answer.\n",
        "            context (List[str]): The retrieved context.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the Ragas metrics.\n",
        "        \"\"\"\n",
        "        data = {\n",
        "            \"question\": [query],\n",
        "            \"answer\": [answer],\n",
        "            \"contexts\": [context]\n",
        "        }\n",
        "        dataset = Dataset.from_dict(data)\n",
        "\n",
        "        result = evaluate(\n",
        "            dataset,\n",
        "            metrics=[\n",
        "            context_precision,\n",
        "            context_recall,\n",
        "            faithfulness,\n",
        "            answer_relevancy,\n",
        "          ] ,\n",
        "        )\n",
        "        return result.to_pandas().iloc[0].to_dict()\n",
        "\n",
        "    def evaluate_dataset(self, dataset: Dataset):\n",
        "        \"\"\"\n",
        "        Evaluate RAG metrics for a dataset of queries, answers, and contexts.\n",
        "\n",
        "        Args:\n",
        "            dataset (Dataset): The dataset containing queries, answers, and contexts.\n",
        "\n",
        "        Returns:\n",
        "            pandas.DataFrame: A DataFrame containing the Ragas metrics for each sample in the dataset.\n",
        "        \"\"\"\n",
        "        return evaluate(\n",
        "            dataset,\n",
        "            metrics=[\n",
        "              context_precision,\n",
        "              context_recall,\n",
        "              faithfulness,\n",
        "              answer_relevancy,\n",
        "          ],\n",
        "        ).to_pandas()"
      ]
    },
    {
      "source": [
        "# Assuming you have a GeminiRAG instance named rag_system\n",
        "# And you have a query, generated answer, and retrieved context\n",
        "\n",
        "query = \"What is the capital of France?\"\n",
        "answer = \"Paris is the capital of France.\"\n",
        "context = [\"France is a country in Europe. Its capital is Paris.\"]\n",
        "\n",
        "# Evaluate a single sample\n",
        "ragas_metrics = rag_system.evaluate_ragas(query, answer, context)\n",
        "print(ragas_metrics)\n",
        "\n",
        "# Assuming you have a dataset in the format of a datasets.Dataset object\n",
        "# with columns \"question\", \"answer\", and \"contexts\"\n",
        "\n",
        "# Evaluate a dataset\n",
        "dataset_ragas_metrics = rag_system.evaluate_dataset(dataset)\n",
        "print(dataset_ragas_metrics)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vBNOfjvDv4Kw"
      },
      "id": "vBNOfjvDv4Kw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c283b600"
      },
      "source": [],
      "id": "c283b600",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######working"
      ],
      "metadata": {
        "id": "tZXZYt7I-J8U"
      },
      "id": "tZXZYt7I-J8U"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7af09dd"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from langchain.vectorstores import FAISS\n",
        "from rank_bm25 import BM25Okapi\n",
        "from typing import List, Tuple\n",
        "from langchain.schema import Document # Import Document\n",
        "\n",
        "class SentenceTransformerRAG:\n",
        "    def __init__(self, documents: List[str], model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
        "        self.documents = documents\n",
        "        # Use SentenceTransformer model\n",
        "        self.embedding_model = SentenceTransformer(model_name)\n",
        "        # Create embeddings for the documents\n",
        "        document_embeddings = self.embedding_model.encode(documents)\n",
        "        # Semantic Index (FAISS)\n",
        "        self.faiss_index = FAISS.from_embeddings([(documents[i], document_embeddings[i]) for i in range(len(documents))], self.embedding_model)\n",
        "        # BM25 Index\n",
        "        self.bm25 = BM25Okapi([doc.split() for doc in documents])\n",
        "\n",
        "    def semantic_search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "        query_embedding = self.embedding_model.encode(query)\n",
        "        # similarity_search_by_vector returns a list of Document objects, not tuples.\n",
        "        docs_with_scores = self.faiss_index.similarity_search_by_vector(query_embedding, k=top_k)\n",
        "        # Extract page_content and score from Document objects\n",
        "        return [(doc.page_content, doc.metadata.get('score', 0.0)) for doc in docs_with_scores]\n",
        "\n",
        "\n",
        "    def bm25_search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "        scores = self.bm25.get_scores(query.split())\n",
        "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "        return [(self.documents[i], scores[i]) for i in top_indices]\n",
        "\n",
        "    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.5) -> List[Tuple[str, float]]:\n",
        "        # Call semantic_search and bm25_search, which now return the correct format\n",
        "        sem_results = dict(self.semantic_search(query, top_k=top_k*2))\n",
        "        bm25_results = dict(self.bm25_search(query, top_k=top_k*2))\n",
        "\n",
        "        # Normalize scores (simple min-max normalization)\n",
        "        sem_scores = np.array(list(sem_results.values()))\n",
        "        bm25_scores = np.array(list(bm25_results.values()))\n",
        "\n",
        "        sem_min, sem_max = sem_scores.min(), sem_scores.max()\n",
        "        bm25_min, bm25_max = bm25_scores.min(), bm25_scores.max()\n",
        "\n",
        "        sem_results_norm = {k: (v - sem_min) / (sem_max - sem_min) if (sem_max - sem_min) != 0 else 0 for k, v in sem_results.items()}\n",
        "        bm25_results_norm = {k: (v - bm25_min) / (bm25_max - bm25_min) if (bm25_max - bm25_min) != 0 else 0 for k, v in bm25_results.items()}\n",
        "\n",
        "\n",
        "        all_docs = set(sem_results.keys()) | set(bm25_results.keys())\n",
        "        hybrid_scores = {\n",
        "            doc: alpha * sem_results_norm.get(doc, 0) + (1 - alpha) * bm25_results_norm.get(doc, 0)\n",
        "            for doc in all_docs\n",
        "        }\n",
        "        ranked = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "        return ranked\n",
        "\n",
        "    def hyde(self, question: str, model=None) -> str:\n",
        "        \"\"\"\n",
        "        Generate a hypothetical answer (HyDE) to the question to use as a retrieval query.\n",
        "        Uses a provided LLM (e.g., Gemini via LangChain).\n",
        "        \"\"\"\n",
        "        if model is None:\n",
        "             raise ValueError(\"An LLM model must be provided for HyDE generation.\")\n",
        "\n",
        "        hypothetical_answer = model.invoke(question) # Assuming the model has an invoke method\n",
        "        return hypothetical_answer\n",
        "\n",
        "    def hyde_hybrid_search(self, question: str, top_k: int = 5, alpha: float = 0.5, model=None) -> List[Tuple[str, float]]:\n",
        "        hypothetical_answer = self.hyde(question, model=model)\n",
        "        return self.hybrid_search(hypothetical_answer, top_k=top_k, alpha=alpha)\n",
        "\n",
        "    def chat(self, prompt: str, llm_model=None) -> Tuple[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Generate a response to the given prompt using RAG.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): The user input prompt.\n",
        "            llm_model: The language model to use for generation.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[str, List[str]]: The generated response and the retrieved context.\n",
        "        \"\"\"\n",
        "        if llm_model is None:\n",
        "             raise ValueError(\"An LLM model must be provided for chat generation.\")\n",
        "\n",
        "        # Example implementation using hybrid search for context\n",
        "        context_docs = self.hybrid_search(prompt, top_k=3) # Retrieve top 3 relevant documents\n",
        "        context = [doc for doc, score in context_docs]\n",
        "\n",
        "        # Combine context and prompt for the LLM\n",
        "        context_text = \"\\n\".join(context)\n",
        "        formatted_prompt = f\"Context: {context_text}\\n\\nQuestion: {prompt}\\n\\nAnswer:\"\n",
        "\n",
        "        response = llm_model.invoke(formatted_prompt) # Assuming the model has an invoke method\n",
        "\n",
        "        return response, context\n",
        "\n",
        "    # Evaluation methods (evaluate_ragas and evaluate_dataset) can be added here\n",
        "    # if needed, similar to the GeminiRAG class."
      ],
      "id": "b7af09dd",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "690c646a",
        "outputId": "b1609f91-648f-4229-efaa-38ea73ab7480"
      },
      "source": [
        "# Sample documents\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Artificial intelligence is a rapidly evolving field.\",\n",
        "    \"Natural language processing is a subfield of AI.\",\n",
        "    \"The sun is a star at the center of our solar system.\",\n",
        "    \"The moon is a natural satellite of the Earth.\"\n",
        "]\n",
        "\n",
        "# Create an instance of the SentenceTransformerRAG class\n",
        "rag_system_st = SentenceTransformerRAG(documents)\n",
        "\n",
        "# Example queries\n",
        "query_ai = \"What is AI?\"\n",
        "query_space = \"Tell me about the moon.\"\n",
        "query_nlp = \"What is NLP?\""
      ],
      "id": "690c646a",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db333916",
        "outputId": "34f57a56-57bf-4308-cee5-598bff2602ce"
      },
      "source": [
        "# Perform semantic search\n",
        "print(f\"Semantic search results for '{query_ai}':\")\n",
        "semantic_results = rag_system_st.semantic_search(query_ai)\n",
        "for doc, score in semantic_results:\n",
        "    print(f\"  Score: {score:.4f}, Document: {doc}\")\n",
        "\n",
        "print(f\"\\nSemantic search results for '{query_space}':\")\n",
        "semantic_results = rag_system_st.semantic_search(query_space)\n",
        "for doc, score in semantic_results:\n",
        "    print(f\"  Score: {score:.4f}, Document: {doc}\")"
      ],
      "id": "db333916",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic search results for 'What is AI?':\n",
            "  Score: 0.0000, Document: Artificial intelligence is a rapidly evolving field.\n",
            "  Score: 0.0000, Document: Natural language processing is a subfield of AI.\n",
            "  Score: 0.0000, Document: The moon is a natural satellite of the Earth.\n",
            "  Score: 0.0000, Document: The sun is a star at the center of our solar system.\n",
            "  Score: 0.0000, Document: The quick brown fox jumps over the lazy dog.\n",
            "\n",
            "Semantic search results for 'Tell me about the moon.':\n",
            "  Score: 0.0000, Document: The moon is a natural satellite of the Earth.\n",
            "  Score: 0.0000, Document: The sun is a star at the center of our solar system.\n",
            "  Score: 0.0000, Document: Artificial intelligence is a rapidly evolving field.\n",
            "  Score: 0.0000, Document: Natural language processing is a subfield of AI.\n",
            "  Score: 0.0000, Document: The quick brown fox jumps over the lazy dog.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69f94000",
        "outputId": "7c5f0988-6cff-491d-f754-865038bde3ea"
      },
      "source": [
        "# Perform BM25 search\n",
        "print(f\"BM25 search results for '{query_ai}':\")\n",
        "bm25_results = rag_system_st.bm25_search(query_ai)\n",
        "for doc, score in bm25_results:\n",
        "    print(f\"  Score: {score:.4f}, Document: {doc}\")\n",
        "\n",
        "print(f\"\\nBM25 search results for '{query_space}':\")\n",
        "bm25_results = rag_system_st.bm25_search(query_space)\n",
        "for doc, score in bm25_results:\n",
        "    print(f\"  Score: {score:.4f}, Document: {doc}\")"
      ],
      "id": "69f94000",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 search results for 'What is AI?':\n",
            "  Score: 0.2319, Document: Artificial intelligence is a rapidly evolving field.\n",
            "  Score: 0.2197, Document: Natural language processing is a subfield of AI.\n",
            "  Score: 0.2087, Document: The moon is a natural satellite of the Earth.\n",
            "  Score: 0.1815, Document: The sun is a star at the center of our solar system.\n",
            "  Score: 0.0000, Document: The quick brown fox jumps over the lazy dog.\n",
            "\n",
            "BM25 search results for 'Tell me about the moon.':\n",
            "  Score: 0.2087, Document: The moon is a natural satellite of the Earth.\n",
            "  Score: 0.2087, Document: The quick brown fox jumps over the lazy dog.\n",
            "  Score: 0.1815, Document: The sun is a star at the center of our solar system.\n",
            "  Score: 0.0000, Document: Natural language processing is a subfield of AI.\n",
            "  Score: 0.0000, Document: Artificial intelligence is a rapidly evolving field.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b872c9cc",
        "outputId": "b124cc7d-5fc8-4c82-bfa1-7fcc60ad356c"
      },
      "source": [
        "# Perform hybrid search\n",
        "print(f\"Hybrid search results for '{query_ai}':\")\n",
        "hybrid_results = rag_system_st.hybrid_search(query_ai)\n",
        "for doc, score in hybrid_results:\n",
        "    print(f\"  Score: {score:.4f}, Document: {doc}\")\n",
        "\n",
        "print(f\"\\nHybrid search results for '{query_space}':\")\n",
        "hybrid_results = rag_system_st.hybrid_search(query_space)\n",
        "for doc, score in hybrid_results:\n",
        "    print(f\"  Score: {score:.4f}, Document: {doc}\")"
      ],
      "id": "b872c9cc",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid search results for 'What is AI?':\n",
            "  Score: 0.5000, Document: Artificial intelligence is a rapidly evolving field.\n",
            "  Score: 0.4737, Document: Natural language processing is a subfield of AI.\n",
            "  Score: 0.4500, Document: The moon is a natural satellite of the Earth.\n",
            "  Score: 0.3913, Document: The sun is a star at the center of our solar system.\n",
            "  Score: 0.0000, Document: The quick brown fox jumps over the lazy dog.\n",
            "\n",
            "Hybrid search results for 'Tell me about the moon.':\n",
            "  Score: 0.5000, Document: The quick brown fox jumps over the lazy dog.\n",
            "  Score: 0.5000, Document: The moon is a natural satellite of the Earth.\n",
            "  Score: 0.4348, Document: The sun is a star at the center of our solar system.\n",
            "  Score: 0.0000, Document: Natural language processing is a subfield of AI.\n",
            "  Score: 0.0000, Document: Artificial intelligence is a rapidly evolving field.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}