{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8391987",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q numpy langchain langchain-google-genai google-generativeai faiss-cpu rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from rank_bm25 import BM25Okapi\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4251c4f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GeminiRAG:\n",
    "    def __init__(self, documents: List[str], embedding_model=None, knoweledge_base=None):\n",
    "        self.documents = documents\n",
    "        # Use Gemini embedding model via LangChain (GoogleGenerativeAIEmbeddings)\n",
    "        self.embedding_model = embedding_model or GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "        # Semantic Index (FAISS)\n",
    "        self.faiss_index = FAISS.from_texts(documents, self.embedding_model)\n",
    "        # BM25 Index\n",
    "        self.bm25 = BM25Okapi([doc.split() for doc in documents])\n",
    "          \"\"\"\n",
    "        Initialize the GeminiRAG instance.\n",
    "\n",
    "        Args:\n",
    "            knowledge_base: Optional knowledge base object to perform RAG.\n",
    "        \"\"\"\n",
    "        self.knowledge_base = knowledge_base\n",
    "        \n",
    "    def semantic_search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        docs_and_scores = self.faiss_index.similarity_search_with_score(query, k=top_k)\n",
    "        return [(doc.page_content, score) for doc, score in docs_and_scores]\n",
    "\n",
    "    def bm25_search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        scores = self.bm25.get_scores(query.split())\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        return [(self.documents[i], scores[i]) for i in top_indices]\n",
    "    \n",
    "    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.5) -> List[Tuple[str, float]]:\n",
    "        sem_results = dict(self.semantic_search(query, top_k=top_k*2))\n",
    "        bm25_results = dict(self.bm25_search(query, top_k=top_k*2))\n",
    "        sem_max = max(sem_results.values(), default=1)\n",
    "        bm25_max = max(bm25_results.values(), default=1)\n",
    "        sem_results_norm = {k: v / sem_max for k, v in sem_results.items()}\n",
    "        bm25_results_norm = {k: v / bm25_max for k, v in bm25_results.items()}\n",
    "        all_docs = set(sem_results) | set(bm25_results)\n",
    "        hybrid_scores = {\n",
    "            doc: alpha * sem_results_norm.get(doc, 0) + (1 - alpha) * bm25_results_norm.get(doc, 0)\n",
    "            for doc in all_docs\n",
    "        }\n",
    "        ranked = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        return ranked\n",
    "\n",
    "    def hyde(self, question: str, model=None) -> str:\n",
    "        \"\"\"\n",
    "        Generate a hypothetical answer (HyDE) to the question to use as a retrieval query.\n",
    "        Uses Gemini via LangChain.\n",
    "        \"\"\"\n",
    "        # Use LangChain's Gemini LLM wrapper (GoogleGenerativeAI)\n",
    "        from langchain.llms import GoogleGenerativeAI\n",
    "        llm = model or GoogleGenerativeAI(model=\"models/gemini-pro\", temperature=0)\n",
    "        hypothetical_answer = llm(question)\n",
    "        return hypothetical_answer\n",
    "\n",
    "    def hyde_hybrid_search(self, question: str, top_k: int = 5, alpha: float = 0.5, model=None) -> List[Tuple[str, float]]:\n",
    "        hypothetical_answer = self.hyde(question, model=model)\n",
    "        return self.hybrid_search(hypothetical_answer, top_k=top_k, alpha=alpha)\n",
    "\n",
    "    def chat(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response to the given prompt using RAG.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The user input prompt.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        # Example implementation (replace with actual logic)\n",
    "        if self.knowledge_base:\n",
    "            context = self.knowledge_base.retrieve(prompt)\n",
    "            return f\"Context: {context}\\nResponse: This is a generated answer.\"\n",
    "        return \"Response: This is a generated answer.\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
